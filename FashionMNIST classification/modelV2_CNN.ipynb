{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>This is a convolutional neural network model with the following architecture</b>\n",
    "<img src=\"network_modelV2_CNN.jpg\" width=\"1200\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loading data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(root=\"data\", train=True, transform=ToTensor(), download=True)\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, transform=ToTensor(), download=True)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning to work with Conv2d layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 61, 61])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 4, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n",
    "test_image = images[0] # get a single image for testing\n",
    "\n",
    "# test_image =  test_image[2, :, :]\n",
    "# plt.imshow(test_image)\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels = 3,\n",
    "                        out_channels = 10, \n",
    "                        kernel_size = (4,4),\n",
    "                        stride=1, \n",
    "                        padding=0)\n",
    "\n",
    "conv_layer_out = conv_layer(test_image)\n",
    "print(conv_layer_out.shape)\n",
    "conv_layer.state_dict()[\"weight\"].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning to work with nn.MaxPool2d \\\n",
    "other methods of pooling are: \n",
    "* L2 pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 20])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pool_layer = nn.MaxPool2d(kernel_size=(3,3))\n",
    "max_pool_layer(conv_layer_out).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Making Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['block1.0.weight', 'block1.0.bias', 'block1.2.weight', 'block1.2.bias', 'block2.0.weight', 'block2.0.bias', 'block2.2.weight', 'block2.2.bias', 'classifier.1.weight', 'classifier.1.bias'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FashionMNISTV2(nn.Module):\n",
    "    def __init__(self, input_shape:int, hidden_units:int, output_shape:int):\n",
    "        #here, input shape = number of channels in the inputs image \n",
    "        # hidden_units = number of feature maps we want\n",
    "        # output_shape = meaning same as before\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # for linear in_features , we need to calculate the last layer's output shape. in this case it's hidden_units*7*7  \n",
    "            nn.Linear(in_features=hidden_units*7*7  , out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model_2 = FashionMNISTV2(input_shape=1, hidden_units=10, output_shape=10)\n",
    "model_2.to(device)\n",
    "\n",
    "model_2.state_dict().keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTV2',\n",
       " 'model_acc': 13.109025559105431,\n",
       " 'model_loss': tensor(2.3027, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluating the model initialized with random weights and biases\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.1)\n",
    "\n",
    "from helper_func import eval_model\n",
    "eval_model(model_2, test_dataloader, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training in epoch:  0\n",
      "finished training with batch  400\n",
      "finished training with batch  800\n",
      "finished training with batch  1200\n",
      "finished training with batch  1600\n",
      "finished training of epoch  0\n",
      "training in epoch:  1\n",
      "finished training with batch  400\n",
      "finished training with batch  800\n",
      "finished training with batch  1200\n",
      "finished training with batch  1600\n",
      "finished training of epoch  1\n",
      "training in epoch:  2\n",
      "finished training with batch  400\n",
      "finished training with batch  800\n",
      "finished training with batch  1200\n",
      "finished training with batch  1600\n",
      "finished training of epoch  2\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "\n",
    "from helper_func import train_model\n",
    "train_model(model_2, loss_fn, optimizer, train_dataloader, epochs=3, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTV2',\n",
       " 'model_acc': 87.96924920127796,\n",
       " 'model_loss': tensor(0.3234, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper_func import eval_model\n",
    "eval_model(model_2, test_dataloader, loss_fn, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
