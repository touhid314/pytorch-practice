{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear nn applied on the fashionMNIST dataset classification problem. \\\n",
    "<a>https://www.youtube.com/watch?v=V_xro1bcAuA&t=58680s</a>\\\n",
    "\n",
    "<b>Network architecture</b>\\\n",
    "<img src=\"network_modelV0.jpg\" width=\"700\"/>\n",
    "\n",
    "<i>NOTE that, the linear layer just applies y = W*x +b. no sigmoid function is applied automatically in pytorch</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Obtaining dataset </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(root=\"data\",\n",
    "                                   train=True,\n",
    "                                   transform=ToTensor(),\n",
    "                                   target_transform=None,\n",
    "                                   download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.FashionMNIST(root=\"data\",\n",
    "                                  train=False,\n",
    "                                  download=True,\n",
    "                                  transform=ToTensor(),\n",
    "                                  target_transform=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing image from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-shirt/top\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, lab = train_data[10]\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "print(train_data.classes[lab])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working with data loader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sandal'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch , label = next(iter(train_dataloader))\n",
    "plt.imshow(batch[1, 0, :, :].squeeze())\n",
    "train_data.classes[label[1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A simple linear model to solve the classification problem</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning to work with the flatten layer\n",
    "flatten_model = nn.Flatten(start_dim=1)\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "img = x[0]\n",
    "# plt.imshow(img)\n",
    "out = flatten_model(img)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.Sequential'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0961,  0.1589, -0.3339, -0.1616, -0.0693,  0.1903, -0.3255, -0.0223,\n",
       "         -0.2894, -0.2555]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning to work with nn.sequential\n",
    "model = nn.Sequential(nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=784, out_features=10), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=10, out_features=10)\n",
    "        )\n",
    "print(type(model))\n",
    "model(out) # out is a flattened image of size [1, 784]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>now creating the model</b>\\\n",
    "model is a subclass of nn.Module\\\n",
    "nn.module provides many useful functions \\\n",
    "to know about them: \n",
    "<a>https://pytorch.org/docs/stable/generated/torch.nn.Module.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model class\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape:int, hidden_units:int, output_shape:int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a [1, 28, 28] image\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the forward pass to predict an img with the initial random weights and biases. \\\n",
    "<b>NOTE THAT: number of neurons in a layer = number of out_features of that layer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4276,  0.0733, -0.0072, -0.1394,  0.1099, -0.4281,  0.3358, -0.1715,\n",
      "         -0.0980, -0.2569]], grad_fn=<AddmmBackward0>)\n",
      "odict_keys(['layer_stack.1.weight', 'layer_stack.1.bias', 'layer_stack.2.weight', 'layer_stack.2.bias'])\n",
      "layer1 weights:  tensor([[-0.0032, -0.0113,  0.0072,  ..., -0.0196,  0.0197, -0.0034],\n",
      "        [ 0.0313, -0.0054, -0.0051,  ..., -0.0190,  0.0262,  0.0199],\n",
      "        [-0.0121,  0.0110, -0.0136,  ...,  0.0266, -0.0183, -0.0290],\n",
      "        ...,\n",
      "        [-0.0316, -0.0009,  0.0130,  ...,  0.0124, -0.0142,  0.0123],\n",
      "        [ 0.0264,  0.0104, -0.0207,  ..., -0.0347,  0.0079,  0.0356],\n",
      "        [-0.0221,  0.0097, -0.0309,  ..., -0.0309, -0.0080, -0.0345]])\n",
      "layer1 weights:  torch.Size([10, 784])\n",
      "layer1 bias:  torch.Size([10])\n",
      "layer2 weights:  torch.Size([10, 10])\n",
      "layer2 bias:  torch.Size([10])\n",
      "total params: 7960\n"
     ]
    }
   ],
   "source": [
    "model_0 = FashionMNISTModelV0(input_shape=784, hidden_units=10, output_shape=len(train_data.classes))\n",
    "model_0.to('cpu') #telling torch for running the model on cpu \n",
    "\n",
    "print(model_0.forward(img)) #predict output for an img\n",
    "#see the weights and biases of the layers\n",
    "#note that the weights and biases are randomly assigned as of now\n",
    "print(model_0.state_dict().keys())\n",
    "\n",
    "layer1weights = model_0.state_dict()[\"layer_stack.1.weight\"]\n",
    "print(\"layer1 weights: \", layer1weights)\n",
    "\n",
    "print(\"layer1 weights: \", layer1weights.shape) \n",
    "print(\"layer1 bias: \", model_0.state_dict()[\"layer_stack.1.bias\"].shape)\n",
    "print(\"layer2 weights: \", model_0.state_dict()[\"layer_stack.2.weight\"].shape)\n",
    "print(\"layer2 bias: \", model_0.state_dict()[\"layer_stack.2.bias\"].shape)\n",
    "\n",
    "total_params = sum(p.numel() for p in model_0.parameters())\n",
    "print(\"total params:\" , total_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Required functions to know: </b> \\\n",
    "<i>loss_fn = nn.CrossEntropyLoss()</i> \\\n",
    "loss = loss_fn(model_prediction, known_prediction) \\\n",
    "\\\n",
    "<i>loss.backward()</i> \\\n",
    "performs gradient of loss function wrt all parameters(weights and biases) of the network\\\n",
    "\\\n",
    "<i> optimizer = torch.optim.SGD(params=model_name.parameters(). lr = 0.1) </i>\\\n",
    "\n",
    "optimizer.zero_grad()\\\n",
    "optimizer.step() -> updates the weights and biases of the model, according to the calculated gradient.\\\n",
    "\n",
    "\n",
    "<b>Why use model(data), instead of model.forward(data)?</b>\\\n",
    "When you call the model directly, the internal __call__ function is used.\n",
    "This function manages all registered hooks and calls forward afterwards.\n",
    "That’s also the reason you should call the model directly (and not model.forward(x)), because otherwise your hooks might not work etc.\\\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> the shape of the inputs to the loss_fn </b> \\\n",
    "In PyTorch, nn.CrossEntropyLoss() expects the input y_pred to have a shape of (batch_size, num_classes) and the target y_original to have a shape of (batch_size,).\n",
    "\n",
    "In your case, y_pred has a shape of [32, 10], which means you have a batch of 32 samples and each sample has 10 predicted class scores. y_original has a shape of [32], indicating the target class labels for the corresponding samples.\n",
    "\n",
    "The nn.CrossEntropyLoss() function will automatically handle the necessary computations to compute the loss between y_pred and y_original. It will apply the softmax function to y_pred internally to obtain class probabilities and then calculate the cross-entropy loss between the predicted probabilities and the target labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:green\">-----------Training our model-----</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training in epoch:  0\n",
      "y_pred torch.Size([32, 10])\n",
      "y tensor([8, 4, 3, 1, 6, 6, 2, 9, 8, 9, 0, 2, 7, 3, 1, 3, 1, 3, 0, 7, 1, 3, 0, 6,\n",
      "        9, 8, 5, 7, 1, 3, 9, 3]) torch.Size([32])\n",
      "finished training with batch  400\n",
      "finished training with batch  800\n",
      "finished training with batch  1200\n",
      "finished training with batch  1600\n",
      "finished training of epoch  0\n",
      "training in epoch:  1\n",
      "y_pred torch.Size([32, 10])\n",
      "y tensor([0, 3, 1, 5, 3, 7, 8, 2, 8, 4, 7, 0, 3, 9, 9, 8, 4, 5, 4, 2, 7, 6, 0, 1,\n",
      "        2, 2, 5, 0, 3, 9, 3, 0]) torch.Size([32])\n",
      "finished training with batch  400\n",
      "finished training with batch  800\n",
      "finished training with batch  1200\n",
      "finished training with batch  1600\n",
      "finished training of epoch  1\n",
      "training in epoch:  2\n",
      "y_pred torch.Size([32, 10])\n",
      "y tensor([6, 9, 0, 9, 2, 9, 9, 5, 8, 8, 7, 6, 0, 5, 9, 0, 5, 3, 6, 0, 4, 4, 2, 3,\n",
      "        2, 6, 9, 1, 9, 2, 4, 8]) torch.Size([32])\n",
      "finished training with batch  400\n",
      "finished training with batch  800\n",
      "finished training with batch  1200\n",
      "finished training with batch  1600\n",
      "finished training of epoch  2\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"training in epoch: \", epoch)\n",
    "    train_loss = 0\n",
    "\n",
    "    batch_count = 0\n",
    "    for X, y in train_dataloader:\n",
    "        batch_count += 1\n",
    "        # X = a batch of 32 images\n",
    "        # y = the corresponding labels for the images\n",
    "        model_0.train() #setting our model to train mode. whether we are testing or training is needed to be known by some layers but not all\n",
    "        #1 forward pass\n",
    "        y_pred = model_0(X) \n",
    "        #2 loss for the batch\n",
    "        if(batch_count == 1):\n",
    "            print(\"y_pred\", y_pred.shape)\n",
    "            print(\"y\",y, y.shape)\n",
    "            \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        #3 optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        #4 backward()\n",
    "        loss.backward() #the backward function doesn't work without a forward pass prior to its call. hence we had to calulate loss. i guess the forward pass creates the necessary computational graph for gradient calculation\n",
    "        #5 update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_count % 400 == 0):\n",
    "            print(\"finished training with batch \", batch_count)\n",
    "\n",
    "    print(\"finished training of epoch \", epoch)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we now have a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original label:  Coat\n",
      "predicted label:  Coat\n"
     ]
    }
   ],
   "source": [
    "model_0.eval() #letting pytorch know that we will test now, not train\n",
    "#take an image from the dataset\n",
    "X_test, y_test =  next(iter(test_dataloader))\n",
    "# X_test = batch of 32 images for test \n",
    "# y_test = corresponding labels \n",
    "test_img = X_test[10] #one img out of the batch, taken out for testing\n",
    "test_img_label = y_test[10]\n",
    "classes = train_data.classes\n",
    "print('original label: ', classes[test_img_label])\n",
    "\n",
    "test_img_pred = model_0(test_img).argmax(dim=1)\n",
    "print('predicted label: ', classes[test_img_pred]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have seen that the model works properly for some randomly picked images, \\\n",
    "let's now find output for all images of the test_dataloader and compare it with the original_label to find the overall accuracy \n",
    "\n",
    "<b> accuracy = (no. of correct output / no. of total outputs) * 100 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_acc': 82.80750798722045,\n",
       " 'model_loss': tensor(0.4743, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a function for evaluating any model\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               test_dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module):\n",
    "    model.train(mode=False) #same as model.eval()\n",
    "    \n",
    "    batch_size = test_dataloader.batch_size\n",
    "    # going through the batches in test_dataloader\n",
    "    correct_pred = 0\n",
    "    total_loss = 0\n",
    "    for X_test, y_test in test_dataloader:\n",
    "        # X_test = batch of 32 image \n",
    "        # y_test = corresponding labels\n",
    "\n",
    "        #accuracy\n",
    "        y_pred = model(X_test)\n",
    "        correct_pred += torch.eq(y_pred.argmax(dim=1), y_test).sum().item()\n",
    "        \n",
    "        #loss\n",
    "        loss = loss_fn(y_pred, y_test)\n",
    "        total_loss += loss\n",
    "        \n",
    "    accuracy = (correct_pred/((len(test_dataloader))*(batch_size) ))*100\n",
    "    total_loss = total_loss/ (len(test_dataloader))\n",
    "\n",
    "    # returning a dictionary\n",
    "    return {\"model_name\":model.__class__.__name__,\n",
    "            \"model_acc\":accuracy,\n",
    "            \"model_loss\":total_loss}\n",
    "\n",
    "\n",
    "eval_model(model_0, test_dataloader, loss_fn = loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>------DONE WITH CREATING THE MODEL-----</b></i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
