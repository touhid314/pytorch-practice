{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear nn applied on the fashionMNIST dataset classification problem. \\\n",
    "<a>https://www.youtube.com/watch?v=V_xro1bcAuA&t=58680s</a>\\\n",
    "\n",
    "<b>Network architecture</b>\\\n",
    "<img src=\"network_modelV0.jpg\" width=\"700\"/>\n",
    "\n",
    "<i>NOTE that, the linear layer just applies y = W*x +b. no sigmoid function is applied automatically in pytorch</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Obtaining dataset </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(root=\"data\",\n",
    "                                   train=True,\n",
    "                                   transform=ToTensor(),\n",
    "                                   target_transform=None,\n",
    "                                   download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.FashionMNIST(root=\"data\",\n",
    "                                  train=False,\n",
    "                                  download=True,\n",
    "                                  transform=ToTensor(),\n",
    "                                  target_transform=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing image from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-shirt/top\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, lab = train_data[10]\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "print(train_data.classes[lab])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working with data loader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sandal'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch , label = next(iter(train_dataloader))\n",
    "plt.imshow(batch[1, 0, :, :].squeeze())\n",
    "train_data.classes[label[1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A simple linear model to solve the classification problem</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning to work with the flatten layer\n",
    "flatten_model = nn.Flatten(start_dim=1)\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "img = x[0]\n",
    "# plt.imshow(img)\n",
    "out = flatten_model(img)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.Sequential'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0961,  0.1589, -0.3339, -0.1616, -0.0693,  0.1903, -0.3255, -0.0223,\n",
       "         -0.2894, -0.2555]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning to work with nn.sequential\n",
    "model = nn.Sequential(nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=784, out_features=10), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=10, out_features=10)\n",
    "        )\n",
    "print(type(model))\n",
    "model(out) # out is a flattened image of size [1, 784]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>now creating the model</b>\\\n",
    "model is a subclass of nn.Module\\\n",
    "nn.module provides many useful functions \\\n",
    "to know about them: \n",
    "<a>https://pytorch.org/docs/stable/generated/torch.nn.Module.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model class\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape:int, hidden_units:int, output_shape:int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a [1, 28, 28] image\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the forward pass to predict an img with the initial random weights and biases. \\\n",
    "<b>NOTE THAT: number of neurons in a layer = number of out_features of that layer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4276,  0.0733, -0.0072, -0.1394,  0.1099, -0.4281,  0.3358, -0.1715,\n",
      "         -0.0980, -0.2569]], grad_fn=<AddmmBackward0>)\n",
      "odict_keys(['layer_stack.1.weight', 'layer_stack.1.bias', 'layer_stack.2.weight', 'layer_stack.2.bias'])\n",
      "layer1 weights:  tensor([[-0.0032, -0.0113,  0.0072,  ..., -0.0196,  0.0197, -0.0034],\n",
      "        [ 0.0313, -0.0054, -0.0051,  ..., -0.0190,  0.0262,  0.0199],\n",
      "        [-0.0121,  0.0110, -0.0136,  ...,  0.0266, -0.0183, -0.0290],\n",
      "        ...,\n",
      "        [-0.0316, -0.0009,  0.0130,  ...,  0.0124, -0.0142,  0.0123],\n",
      "        [ 0.0264,  0.0104, -0.0207,  ..., -0.0347,  0.0079,  0.0356],\n",
      "        [-0.0221,  0.0097, -0.0309,  ..., -0.0309, -0.0080, -0.0345]])\n",
      "layer1 weights:  torch.Size([10, 784])\n",
      "layer1 bias:  torch.Size([10])\n",
      "layer2 weights:  torch.Size([10, 10])\n",
      "layer2 bias:  torch.Size([10])\n",
      "total params: 7960\n"
     ]
    }
   ],
   "source": [
    "model_0 = FashionMNISTModelV0(input_shape=784, hidden_units=10, output_shape=len(train_data.classes))\n",
    "model_0.to('cpu') #telling torch for running the model on cpu \n",
    "\n",
    "print(model_0.forward(img)) #predict output for an img\n",
    "#see the weights and biases of the layers\n",
    "#note that the weights and biases are randomly assigned as of now\n",
    "print(model_0.state_dict().keys())\n",
    "\n",
    "layer1weights = model_0.state_dict()[\"layer_stack.1.weight\"]\n",
    "print(\"layer1 weights: \", layer1weights)\n",
    "\n",
    "print(\"layer1 weights: \", layer1weights.shape) \n",
    "print(\"layer1 bias: \", model_0.state_dict()[\"layer_stack.1.bias\"].shape)\n",
    "print(\"layer2 weights: \", model_0.state_dict()[\"layer_stack.2.weight\"].shape)\n",
    "print(\"layer2 bias: \", model_0.state_dict()[\"layer_stack.2.bias\"].shape)\n",
    "\n",
    "total_params = sum(p.numel() for p in model_0.parameters())\n",
    "print(\"total params:\" , total_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Required functions to know: </b> \\\n",
    "<i>loss_fn = nn.CrossEntropyLoss()</i> \\\n",
    "loss = loss_fn(model_prediction, known_prediction) \\\n",
    "\\\n",
    "<i>loss.backward()</i> \\\n",
    "performs gradient of loss function wrt all parameters(weights and biases) of the network\\\n",
    "\\\n",
    "<i> optimizer = torch.optim.SGD(params=model_name.parameters(). lr = 0.1) </i>\\\n",
    "\n",
    "optimizer.zero_grad()\\\n",
    "optimizer.step() -> updates the weights and biases of the model, according to the calculated gradient.\\\n",
    "\n",
    "\n",
    "<b>Why use model(data), instead of model.forward(data)?</b>\\\n",
    "When you call the model directly, the internal __call__ function is used.\n",
    "This function manages all registered hooks and calls forward afterwards.\n",
    "Thatâ€™s also the reason you should call the model directly (and not model.forward(x)), because otherwise your hooks might not work etc.\\\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> the shape of the inputs to the loss_fn </b> \\\n",
    "In PyTorch, nn.CrossEntropyLoss() expects the input y_pred to have a shape of (batch_size, num_classes) and the target y_original to have a shape of (batch_size,).\n",
    "\n",
    "In your case, y_pred has a shape of [32, 10], which means you have a batch of 32 samples and each sample has 10 predicted class scores. y_original has a shape of [32], indicating the target class labels for the corresponding samples.\n",
    "\n",
    "The nn.CrossEntropyLoss() function will automatically handle the necessary computations to compute the loss between y_pred and y_original. It will apply the softmax function to y_pred internally to obtain class probabilities and then calculate the cross-entropy loss between the predicted probabilities and the target labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:green\">-----------Training our model-----</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training in epoch:  0\n",
      "y_pred torch.Size([32, 10])\n",
      "y tensor([8, 4, 3, 1, 6, 6, 2, 9, 8, 9, 0, 2, 7, 3, 1, 3, 1, 3, 0, 7, 1, 3, 0, 6,\n",
      "        9, 8, 5, 7, 1, 3, 9, 3]) torch.Size([32])\n",
      "finished training with batch  400\n",
      "finished training with batch  800\n",
      "finished training with batch  1200\n",
      "finished training with batch  1600\n",
      "finished training of epoch  0\n",
      "training in epoch:  1\n",
      "y_pred torch.Size([32, 10])\n",
      "y tensor([0, 3, 1, 5, 3, 7, 8, 2, 8, 4, 7, 0, 3, 9, 9, 8, 4, 5, 4, 2, 7, 6, 0, 1,\n",
      "        2, 2, 5, 0, 3, 9, 3, 0]) torch.Size([32])\n",
      "finished training with batch  400\n",
      "finished training with batch  800\n",
      "finished training with batch  1200\n",
      "finished training with batch  1600\n",
      "finished training of epoch  1\n",
      "training in epoch:  2\n",
      "y_pred torch.Size([32, 10])\n",
      "y tensor([6, 9, 0, 9, 2, 9, 9, 5, 8, 8, 7, 6, 0, 5, 9, 0, 5, 3, 6, 0, 4, 4, 2, 3,\n",
      "        2, 6, 9, 1, 9, 2, 4, 8]) torch.Size([32])\n",
      "finished training with batch  400\n",
      "finished training with batch  800\n",
      "finished training with batch  1200\n",
      "finished training with batch  1600\n",
      "finished training of epoch  2\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"training in epoch: \", epoch)\n",
    "    train_loss = 0\n",
    "\n",
    "    batch_count = 0\n",
    "    for X, y in train_dataloader:\n",
    "        batch_count += 1\n",
    "        # X = a batch of 32 images\n",
    "        # y = the corresponding labels for the images\n",
    "        model_0.train() #setting our model to train mode. whether we are testing or training is needed to be known by some layers but not all\n",
    "        #1 forward pass\n",
    "        y_pred = model_0(X) \n",
    "        #2 loss for the batch\n",
    "        if(batch_count == 1):\n",
    "            print(\"y_pred\", y_pred.shape)\n",
    "            print(\"y\",y, y.shape)\n",
    "            \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        #3 optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        #4 backward()\n",
    "        loss.backward() #the backward function doesn't work without a forward pass prior to its call. hence we had to calulate loss. i guess the forward pass creates the necessary computational graph for gradient calculation\n",
    "        #5 update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_count % 400 == 0):\n",
    "            print(\"finished training with batch \", batch_count)\n",
    "\n",
    "    print(\"finished training of epoch \", epoch)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we now have a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original label:  Coat\n",
      "predicted label:  Coat\n"
     ]
    }
   ],
   "source": [
    "model_0.eval() #letting pytorch know that we will test now, not train\n",
    "#take an image from the dataset\n",
    "X_test, y_test =  next(iter(test_dataloader))\n",
    "# X_test = batch of 32 images for test \n",
    "# y_test = corresponding labels \n",
    "test_img = X_test[10] #one img out of the batch, taken out for testing\n",
    "test_img_label = y_test[10]\n",
    "classes = train_data.classes\n",
    "print('original label: ', classes[test_img_label])\n",
    "\n",
    "test_img_pred = model_0(test_img).argmax(dim=1)\n",
    "print('predicted label: ', classes[test_img_pred]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have seen that the model works properly for some randomly picked images, \\\n",
    "let's now find output for all images of the test_dataloader and compare it with the original_label to find the overall accuracy \n",
    "\n",
    "<b> accuracy = (no. of correct output / no. of total outputs) * 100 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_acc': 82.80750798722045,\n",
       " 'model_loss': tensor(0.4743, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a function for evaluating any model\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               test_dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module):\n",
    "    model.train(mode=False) #same as model.eval()\n",
    "    \n",
    "    batch_size = test_dataloader.batch_size\n",
    "    # going through the batches in test_dataloader\n",
    "    correct_pred = 0\n",
    "    total_loss = 0\n",
    "    for X_test, y_test in test_dataloader:\n",
    "        # X_test = batch of 32 image \n",
    "        # y_test = corresponding labels\n",
    "\n",
    "        #accuracy\n",
    "        y_pred = model(X_test)\n",
    "        correct_pred += torch.eq(y_pred.argmax(dim=1), y_test).sum().item()\n",
    "        \n",
    "        #loss\n",
    "        loss = loss_fn(y_pred, y_test)\n",
    "        total_loss += loss\n",
    "        \n",
    "    accuracy = (correct_pred/((len(test_dataloader))*(batch_size) ))*100\n",
    "    total_loss = total_loss/ (len(test_dataloader))\n",
    "\n",
    "    # returning a dictionary\n",
    "    return {\"model_name\":model.__class__.__name__,\n",
    "            \"model_acc\":accuracy,\n",
    "            \"model_loss\":total_loss}\n",
    "\n",
    "\n",
    "eval_model(model_0, test_dataloader, loss_fn = loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>------DONE WITH CREATING THE MODEL-----</b></i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
